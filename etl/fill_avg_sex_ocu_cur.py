"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è –ë–î –¥–∞–Ω–Ω—ã–º–∏ –æ —Å—Ä–µ–¥–Ω–µ–π –º–µ—Å—è—á–Ω–æ–π –ó–ü –ø–æ –ø–æ–ª—É –∏ —Ä–æ–¥—É –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
–°–∫–∞—á–∞–ª –¥–∞–Ω–Ω—ã–µ –æ—Ç—Å—é–¥–∞: https://rshiny.ilo.org/dataexplorer06/?lang=en&segment=indicator&id=EAR_4MTH_SEX_OCU_CUR_NB_A
–§–∞–π–ª –≤ data: EAR_4MTH_SEX_OCU_CUR_NB_A-filtered-2025-11-20
"""

import asyncio
import csv
from pathlib import Path
from typing import Dict, List, Literal, cast

import pandas as pd
from sqlalchemy.ext.asyncio import AsyncSession

from etl.utils import (
    compare_country_list_by_column,
    get_or_create_metric_cached,
    get_or_create_period_cached,
    get_or_create_series_cached,
    validate_csv_structure,
)
from src.core.config.logging import setup_logger_to_file
from src.core.database.database import AsyncSessionLocal
from src.ms_location.dto import CountryGetDTO
from src.ms_location.models import CountryModel
from src.ms_metric.enums import CategoryMetricEnum, PeriodTypeEnum, TypeDataEnum
from src.ms_metric.models import (
    MetricDataModel,
    MetricInfoModel,
    MetricPeriodModel,
    MetricSeriesModel,
)


logger = setup_logger_to_file()


# ============================================================
#                    CONFIG
# ============================================================
# –£–∫–∞–∂–∏ —Ç—Ä–µ–±—É–µ–º—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –≤ CSV —Ñ–∞–π–ª–µ
REQUIRED_COLUMNS = {
    "ref_area.label",
    "source.label",
    "indicator.label",
    "sex.label",
    "classif1.label",
    "classif2.label",
    "time",
    "obs_value",
    "obs_status.label",
    "note_classif.label",
    "note_indicator.label",
    "note_source.label",
}

# –ú–∞–ø–ø–∏–Ω–≥ –∫–æ–¥–æ–≤ –≤ CSV -> —Å–ø–∏—Å–æ–∫
COUNTRY_MAP = {
    "Bolivia (Plurinational State of)": ["Bolivia"],
    "Brunei Darussalam": ["Brunei"],
    "C√¥te d'Ivoire": ["Ivory Coast"],
    "Hong Kong, China": ["Hong Kong"],
    "Lao People's Democratic Republic": ["Laos"],
    "Macao, China": ["Macao"],
    "Occupied Palestinian Territory": ["Palestine"],
    "Republic of Korea": ["South Korea"],
    "Republic of Moldova": ["Moldova"],
    "Russian Federation": ["Russia"],
    "Tanzania, United Republic of": ["Tanzania"],
    "United Kingdom of Great Britain and Northern Ireland": ["Great Britain", "Ireland"],
    "United States of America": ["USA"],
    "Venezuela (Bolivarian Republic of)": ["Venezuela"],
    "Viet Nam": ["Vietnam"],
    "Marshall Islands": ["Marshall Islands"],
    "Australia": ["Australia"],
    "Bermuda": ["Bermuda"],
    "Botswana": ["Botswana"],
    "Bosnia and Herzegovina": ["Bosnia and Herzegovina"],
    "Cura√ßao": ["Cura√ßao"],
    "Congo": ["Congo Republic"],
    "Congo, Democratic Republic of the": ["DR Congo"],
}


# ============================================================
#                 –û–°–ù–û–í–ù–û–ô –ò–ú–ü–û–†–¢ CAR
# ============================================================
async def import_csv(session: AsyncSession, file_path: Path, batch_size: int = 50, delimiter: str = ","):
    """–ò–º–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫ –∏–∑ CSV –≤ –ë–î.

    –û—Å–Ω–æ–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞:
    1. –°–æ–ø–æ—Å—Ç–∞–≤–ª—è–µ–º —Å—Ç—Ä–∞–Ω—É
    2. –°–æ–∑–¥–∞—ë–º (–∏–ª–∏ –±–µ—Ä—ë–º) –º–µ—Ç—Ä–∏–∫—É MetricInfoModel
    3. –°–æ–∑–¥–∞—ë–º (–∏–ª–∏ –±–µ—Ä—ë–º) —Å–µ—Ä–∏—é MetricSeriesModel
    4. –°–æ–∑–¥–∞—ë–º (–∏–ª–∏ –±–µ—Ä—ë–º) –ø–µ—Ä–∏–æ–¥ MetricPeriodModel
    5. –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–ø–∏—Å—å MetricDataModel
    """

    # –ö—ç—à
    country_cache: Dict[str, int] = {}
    metric_cache: Dict[str, MetricInfoModel] = {}
    series_cache: Dict[str, MetricSeriesModel] = {}
    period_cache: Dict[str, MetricPeriodModel] = {}

    buffer: List[MetricDataModel] = []  # –ë—É—Ñ–µ—Ä –¥–ª—è batch insert
    total_inserted = 0

    # –û—Ç–∫—Ä—ã–≤–∞—é CSV —Ñ–∞–π–ª
    with open(file_path, encoding="utf-8") as f:

        # –ß–∏—Ç–∞–µ–º –ø–µ—Ä–≤—É—é —Å—Ç—Ä–æ–∫—É (–∑–∞–≥–æ–ª–æ–≤–∫–∏) –∏ —á–∏—Å—Ç–∏–º –∏—Ö
        raw_header = f.readline().strip().split(delimiter)
        header = [col.strip().replace("\ufeff", "").replace('"', "").replace("'", "") for col in raw_header]

        # === –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ ===
        validate_csv_structure(header, REQUIRED_COLUMNS)

        # –ü–µ—Ä–µ—Ö–æ–¥–∏–º –≤ –Ω–∞—á–∞–ª–æ —Ñ–∞–π–ª–∞ –∏ —Å–æ–∑–¥–∞—ë–º DictReader —Å —á–∏—Å—Ç—ã–º–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏
        f.seek(0)
        reader = csv.DictReader(f, fieldnames=header, delimiter=delimiter)
        next(reader)  # –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–µ—Ä–≤—É—é —Å—Ç—Ä–æ–∫—É-–∑–∞–≥–æ–ª–æ–≤–æ–∫ (—É–∂–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞)

        # –ü—Ä–æ—Ö–æ–∂—É—Å—å –ø–æ —Å—Ç—Ä–æ–∫–∞–º —Ñ–∞–π–ª–∞
        for row in reader:

            # --- –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–∞–Ω ---
            country_from_file = row["ref_area.label"].strip()
            list_country_name_eng = COUNTRY_MAP.get(country_from_file, [country_from_file])

            # –ü—Ä–æ—Ö–æ–∂—É—Å—å –ø–æ —Å—Ç—Ä–∞–Ω–∞–º —Å –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç—è–º–∏
            for country_name_eng in list_country_name_eng:

                # === –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç—Ä–∞–Ω—É ===
                if country_name_eng in country_cache:
                    country_id = country_cache[country_name_eng]
                else:
                    country_obj = await CountryModel.get(session, CountryGetDTO(name_eng=country_name_eng))
                    if not country_obj:
                        logger.warning(f"‚ö†Ô∏è –°—Ç—Ä–∞–Ω–∞ '{country_name_eng}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ ‚Äî –ø—Ä–æ–ø—É—Å–∫.")
                        continue
                    country_id = cast(int, country_obj.id)
                    country_cache[country_name_eng] = country_id

                # === –°–æ–∑–¥–∞—é –º–µ—Ç—Ä–∏–∫—É: –æ–ø—Ä–µ–¥–µ–ª—è—é –æ—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–ª—è –∏ –≤—ã–∑—ã–≤–∞—é –º–µ—Ç–æ–¥ get_or_create_metric_cached ===
                slug = row["indicator.label"].lower().replace(" ", "_").strip()
                unique_key_metric = slug
                name = row["indicator.label"]
                description = None
                category = CategoryMetricEnum.ECONOMY
                source_name = "ILOSTAT"
                source_url = (
                    "https://rshiny.ilo.org/dataexplorer06/?lang=en&segment=indicator&id=EAR_4MTH_SEX_OCU_CUR_NB_A"
                )
                type_data = TypeDataEnum.FLOAT
                add_info = None

                metric = await get_or_create_metric_cached(
                    cache=metric_cache,
                    unique_key=unique_key_metric,
                    session=session,
                    slug=slug,
                    name=name,
                    description=description,
                    category=category,
                    source_name=source_name,
                    source_url=source_url,
                    type_data=type_data,
                    add_info=add_info,
                )
                metric_id = cast(int, metric.id)

                # === –°–æ–∑–¥–∞—é —Å–µ—Ä–∏—é: –æ–ø—Ä–µ–¥–µ–ª—è—é –æ—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–ª—è –∏ –≤—ã–∑—ã–≤–∞—é –º–µ—Ç–æ–¥ get_or_create_series_cached ===
                gender = row["sex.label"]
                unit = row["classif2.label"].strip()
                profession = row["classif1.label"].strip()
                source = row["source.label"].strip()
                obs_status = row["obs_status.label"].strip()
                note_indicator = row["note_indicator.label"].strip()
                note_source = row["note_source.label"].strip()

                unique_key_series = (
                    f"{metric_id} {unit} {gender} {profession} {source} {obs_status} {note_indicator} {note_source}"
                )
                add_info = {
                    "unit": unit,
                    "gender": gender,
                    "profession": profession,
                    "source": source,
                    "obs_status": obs_status,
                    "note_indicator": note_indicator,
                    "note_source": note_source,
                }

                series = await get_or_create_series_cached(
                    cache=series_cache,
                    unique_key=unique_key_series,
                    session=session,
                    metric_id=metric_id,
                    add_info=add_info,
                )
                series_id = cast(int, series.id)

                # === –°–æ–∑–¥–∞—é –ø–µ—Ä–∏–æ–¥: –æ–ø—Ä–µ–¥–µ–ª—è—é –æ—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–ª—è –∏ –≤—ã–∑—ã–≤–∞—é –º–µ—Ç–æ–¥ get_or_create_period_cached ===
                year = int(row["time"])
                period_type = PeriodTypeEnum.YEARLY
                unique_key_period = f"{metric_id}_{series_id}_{year}"

                period = await get_or_create_period_cached(
                    cache=period_cache,
                    unique_key=unique_key_period,
                    session=session,
                    series_id=series_id,
                    period_type=period_type,
                    year=year,
                )
                period_id = cast(int, period.id)

                # === –°–æ–∑–¥–∞—é –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è –º–µ—Ç—Ä–∏–∫–∏ ===
                try:
                    value = float(row["obs_value"].strip().replace(",", ".")) if row["obs_value"].strip() else None
                except ValueError:
                    logger.warning(f"‚ö†Ô∏è –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ obs_value: {row['obs_value']}")
                    continue

                if value is None:
                    continue

                buffer.append(
                    MetricDataModel(
                        series_id=series_id,
                        period_id=period_id,
                        country_id=country_id,
                        value_float=value,
                        add_info=None,
                    )
                )

                # === –ú–∞—Å—Å–æ–≤–∞—è –≤—Å—Ç–∞–≤–∫–∞ ===
                if len(buffer) >= batch_size:
                    session.add_all(buffer)
                    await session.commit()
                    total_inserted += len(buffer)
                    logger.info(f"üíæ –ú–∞—Å—Å–æ–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ ‚Äî {total_inserted} –∑–∞–ø–∏—Å–µ–π")
                    buffer.clear()

    # –§–∏–Ω–∞–ª—å–Ω—ã–π commit –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è –∑–∞–ø–∏—Å–µ–π
    if buffer:
        session.add_all(buffer)
        await session.commit()
        total_inserted += len(buffer)
        logger.info(f"üíæ –§–∏–Ω–∞–ª—å–Ω—ã–π commit ‚Äî {total_inserted} –∑–∞–ø–∏—Å–µ–π")

    logger.info(f"\n‚úÖ –ò–º–ø–æ—Ä—Ç –∑–∞–≤–µ—Ä—à—ë–Ω. –í—Å–µ–≥–æ –¥–æ–±–∞–≤–ª–µ–Ω–æ: {total_inserted} –∑–∞–ø–∏—Å–µ–π.")


# ================= Main =================
async def main(mode: Literal["check", "import"]):

    file_path = Path("data/EAR_4MTH_SEX_OCU_CUR_NB_A-filtered-2025-11-20-2.csv")

    async with AsyncSessionLocal() as session:
        if mode == "check":
            df = pd.read_csv(file_path, sep=",")
            values = df["ref_area.label"].tolist()
            await compare_country_list_by_column(
                session=session,
                list_string=values,
                column_name="name_eng",
                country_map=COUNTRY_MAP,
            )
            return

        elif mode == "import":
            await import_csv(session=session, file_path=file_path, batch_size=100, delimiter=";")
            return
