# etl/universal/service_etl.py
"""
–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π ETL –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –∞—Ç—Ä–∏–±—É—Ç–æ–≤-—Ñ–∏–ª—å—Ç—Ä–æ–≤
"""

import asyncio
import csv
import time
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
from datetime import datetime
from pathlib import Path
from typing import Dict, List, cast

import numpy as np
import pandas as pd
from sqlalchemy.ext.asyncio import AsyncSession

from etl.universal.config_schema import ETLConfig, GeographyLevelEnum
from etl.universal.service_db_etl import DB_ServiceUniversalETL
from etl.universal.session_manager import session_manager
from src.core.config.logging import setup_logger_to_file
from src.ms_metric.models import MetricDataNewModel


logger = setup_logger_to_file()


class UniversalETL:
    """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π ETL –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä"""

    def __init__(self, config: ETLConfig):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""

        self.config = config
        self.session: AsyncSession
        self.db_service: DB_ServiceUniversalETL
        self.executor = ThreadPoolExecutor(max_workers=self.config.max_workers or 8)
        self.stats = {
            "total_rows": 0,
            "processed_rows": 0,
            "skipped_rows": 0,
            "errors": [],
            "start_time": None,
            "end_time": None,
        }

    async def __aenter__(self):
        """–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è –∞–≤—Ç–æ-—Å–æ–∑–¥–∞–Ω–∏—è —Å–µ—Å—Å–∏–∏ —Å –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–æ–π"""

        logger.info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ETL —Å–µ—Å—Å–∏–∏ —Å –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–æ–π...")

        try:
            await session_manager.initialize()
            logger.info("Session manager –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")

            self._session_context = session_manager.get_session()
            self.session = await self._session_context.__aenter__()
            logger.info(f"–°–µ—Å—Å–∏—è —Å–æ–∑–¥–∞–Ω–∞: {self.session}")

            self.db_service = DB_ServiceUniversalETL(self.session, config=self.config)
            logger.info("DB —Å–µ—Ä–≤–∏—Å —Å–æ–∑–¥–∞–Ω")

            # –û—á–∏—â–∞–µ–º –≤—Å–µ –∫—ç—à–∏
            await self.db_service.clear_all_caches()

            # –ü—Ä–µ–¥–∑–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –≤ –∫—ç—à –ø–µ—Ä–µ–¥ –Ω–∞—á–∞–ª–æ–º –æ–±—Ä–∞–±–æ—Ç–∫–∏
            logger.info("\nüöÄ –ù–∞—á–∏–Ω–∞–µ–º –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫—É –¥–∞–Ω–Ω—ã—Ö...")

            # –ü—Ä–µ–¥–∑–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω—ã
            await self.db_service.preload_countries(column_name=self.config.country_column)

            # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç—Ä–∏–∫—É –¥–ª—è –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∏ —Å–µ—Ä–∏–π –∏ –ø–µ—Ä–∏–æ–¥–æ–≤
            metric = await self.db_service.get_or_create_metric(self.config.metric)
            await self.session.commit()

            # –ü—Ä–µ–¥–∑–∞–≥—Ä—É–∂–∞–µ–º —Ç–∏–ø—ã –∞—Ç—Ä–∏–±—É—Ç–æ–≤
            if len(self.config.metric.attributes) > 0:
                await self.db_service.preload_attribute_types()
                await self.db_service.preload_attribute_values()

            # –ü—Ä–µ–¥–∑–∞–≥—Ä—É–∂–∞–µ–º —Å–µ—Ä–∏–∏ –∏ –ø–µ—Ä–∏–æ–¥—ã –¥–ª—è —ç—Ç–æ–π –º–µ—Ç—Ä–∏–∫–∏
            await self.db_service.preload_series_for_metric(cast(int, metric.id))
            await self.db_service.preload_periods_for_metric(cast(int, metric.id))

            # –õ–æ–≥–∏—Ä—É–µ–º –Ω–∞—á–∞–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫—ç—à–µ–π
            await self.db_service.log_cache_stats()

            logger.info("‚úÖ –ü—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞\n")

            return self
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ ETL: {e}")
            raise

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """–ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ –º–µ–Ω–µ–¥–∂–µ—Ä–∞ —Å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""

        logger.info("–ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ ETL —Å–µ—Å—Å–∏–∏...")

        # –õ–æ–≥–∏—Ä—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫—ç—à–µ–π
        if hasattr(self, "db_service") and self.db_service:
            await self.db_service.log_cache_stats()

            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            logger.info("\nüìä –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ö–≠–®–ò–†–û–í–ê–ù–ò–Ø:")

            total_hits = 0
            total_misses = 0

            for cache_name in [
                "_country_cache",
                "_city_cache",
                "_metric_cache",
                "_series_cache",
                "_period_cache",
                "_attribute_type_cache",
                "_attribute_value_cache",
            ]:
                cache = getattr(self.db_service, cache_name, None)
                if cache and hasattr(cache, "stats"):
                    stats = cache.stats()
                    total_hits += stats["hits"]
                    total_misses += stats["misses"]

            total_requests = total_hits + total_misses
            if total_requests > 0:
                overall_hit_rate = (total_hits / total_requests) * 100
                logger.info(f"  –í—Å–µ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ –∫—ç—à–∞–º: {total_requests:,}")
                logger.info(f"  –ü–æ–ø–∞–¥–∞–Ω–∏–π: {total_hits:,} ({overall_hit_rate:.1f}%)")
                logger.info(f"  –ü—Ä–æ–º–∞—Ö–æ–≤: {total_misses:,}")

        # –ó–∞–∫—Ä—ã–≤–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä —Å–µ—Å—Å–∏–∏
        if hasattr(self, "_session_context"):
            await self._session_context.__aexit__(exc_type, exc_val, exc_tb)

        # –ó–∞–∫—Ä—ã–≤–∞–µ–º –º–µ–Ω–µ–¥–∂–µ—Ä —Å–µ—Å—Å–∏–π
        try:
            await session_manager.close()
            logger.info("Session manager –∑–∞–∫—Ä—ã—Ç")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–∫—Ä—ã—Ç–∏–∏ session manager: {e}")

    async def check_countries(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Å—Ç—Ä–∞–Ω –≤ CSV —Å –ë–î –∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å"""

        logger.info(f"\n{'='*60}")
        logger.info("–ü–†–û–í–ï–†–ö–ê –°–¢–†–ê–ù –í CSV –§–ê–ô–õ–ï")
        logger.info(f"{'='*60}")

        # –ü—Ä–µ–¥–∑–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω—ã –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
        if not self.db_service._countries_preloaded:
            await self.db_service.preload_countries(column_name=self.config.country_column)

        csv_path = Path(self.config.csv_file)
        if not csv_path.exists():
            logger.error(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {self.config.csv_file}")
            return False

        logger.info(f"CSV —Ñ–∞–π–ª: {csv_path}")
        logger.info(f"–ö–æ–¥–∏—Ä–æ–≤–∫–∞: {self.config.csv_encoding}")
        logger.info(f"–†–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å: {repr(self.config.csv_delimiter)}")
        logger.info(f"–ö–æ–ª–æ–Ω–∫–∞ –¥–ª—è —Å—Ç—Ä–∞–Ω—ã: '{self.config.metric.country_column}'")

        # 1. –ü–æ–ª—É—á–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å—Ç—Ä–∞–Ω—ã –∏–∑ CSV
        countries_in_csv = set()
        with open(csv_path, "r", encoding=self.config.csv_encoding) as f:
            reader = csv.DictReader(f, delimiter=self.config.csv_delimiter)

            # –£–¥–∞–ª—è–µ–º BOM —Å–∏–º–≤–æ–ª—ã –∏–∑ –∏–º–µ–Ω –∫–æ–ª–æ–Ω–æ–∫
            if reader.fieldnames:
                reader.fieldnames = [name.replace("\ufeff", "") for name in reader.fieldnames]
                logger.info(f"–û—á–∏—â–µ–Ω–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏: {reader.fieldnames}")

            if not reader.fieldnames:
                logger.error("‚ùå CSV –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ (fieldnames=None)")
                return False

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –∫–æ–ª–æ–Ω–∫–∞ —Å—Ç—Ä–∞–Ω—ã
            if self.config.metric.country_column not in reader.fieldnames:
                logger.error(f"‚ùå –ö–æ–ª–æ–Ω–∫–∞ '{self.config.metric.country_column}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ CSV!")
                logger.error(f"–î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {reader.fieldnames}")
                return False

            row_count = 0
            for row in reader:
                row_count += 1
                country = row.get(self.config.metric.country_column, "").strip()
                if country:
                    countries_in_csv.add(country)

        logger.info(f"\n–í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫ –ø—Ä–æ—á–∏—Ç–∞–Ω–æ: {row_count}")
        logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(countries_in_csv)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Ç—Ä–∞–Ω –≤ CSV")

        # –í—ã–≤–æ–¥–∏–º –ø–µ—Ä–≤—ã–µ 10 —Å—Ç—Ä–∞–Ω –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
        if countries_in_csv:
            logger.debug(f"–ü—Ä–∏–º–µ—Ä—ã —Å—Ç—Ä–∞–Ω –∏–∑ CSV (–ø–µ—Ä–≤—ã–µ 10): {list(sorted(countries_in_csv))[:10]}")

        # 2. –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —Å—Ç—Ä–∞–Ω—ã –∏–∑ –ë–î
        logger.info("\nüó∫Ô∏è  –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω –∏–∑ –ë–î...")
        all_db_countries = await self.db_service._get_all_countries_from_db()

        if not all_db_countries:
            logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–∞–Ω –∏–∑ –ë–î")
            return False

        logger.info(f"–í—Å–µ–≥–æ —Å—Ç—Ä–∞–Ω –≤ –ë–î: {len(all_db_countries)}")
        logger.info(f"–ü—Ä–∏–º–µ—Ä—ã —Å—Ç—Ä–∞–Ω –∏–∑ –ë–î (–ø–µ—Ä–≤—ã–µ 10): {list(sorted(all_db_countries.keys()))[:10]}")

        # 3. –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥—É—é —Å—Ç—Ä–∞–Ω—É –∏–∑ CSV –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –≤ –ë–î
        countries_found = []
        countries_not_found = []
        mapping_used = []  # –î–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã—Ö –º–∞–ø–ø–∏–Ω–≥–æ–≤
        found_country_ids = set()  # ID —Å—Ç—Ä–∞–Ω, –∫–æ—Ç–æ—Ä—ã–µ –±—ã–ª–∏ –Ω–∞–π–¥–µ–Ω—ã

        logger.info("\nüîç –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä–∞–Ω—ã –∏–∑ CSV –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö...")

        for country_name in sorted(countries_in_csv):
            country_id = await self.db_service.get_country_id(
                country_name=country_name,
                country_mapping=self.config.country_mapping,
                column_name=self.config.country_column,
            )

            if country_id:
                countries_found.append(country_name)
                found_country_ids.add(country_id)

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª—Å—è –ª–∏ –º–∞–ø–ø–∏–Ω–≥
                if country_name in self.config.country_mapping:
                    mapping_used.append(country_name)
            else:
                countries_not_found.append(country_name)

        # 4. –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç—Ä–∞–Ω—ã –∏–∑ –ë–î, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ CSV
        countries_in_db_not_in_csv = []
        for db_country_name, db_country_id in all_db_countries.items():
            if db_country_id not in found_country_ids:
                countries_in_db_not_in_csv.append((db_country_name, db_country_id))

        # 5. –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã CSV ‚Üí –ë–î
        logger.info(f"\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–†–û–í–ï–†–ö–ò –°–¢–†–ê–ù (CSV ‚Üí –ë–î):")
        logger.info(f"   –í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Ç—Ä–∞–Ω –≤ CSV: {len(countries_in_csv)}")
        logger.info(f"   –ù–∞–π–¥–µ–Ω–æ –≤ –ë–î: {len(countries_found)}")
        logger.info(f"   –ù–µ –Ω–∞–π–¥–µ–Ω–æ –≤ –ë–î: {len(countries_not_found)}")
        logger.info(f"   –ü—Ä–æ—Ü–µ–Ω—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è: {(len(countries_found)/len(countries_in_csv))*100:.1f}%")

        # 6. –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ë–î ‚Üí CSV (–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ)
        logger.info(f"\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –û–ë–†–ê–¢–ù–û–ô –°–í–Ø–ó–ò (–ë–î ‚Üí CSV):")
        logger.info(f"   –í—Å–µ–≥–æ —Å—Ç—Ä–∞–Ω –≤ –ë–î: {len(all_db_countries)}")
        logger.info(f"   –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ –≤ CSV: {len(found_country_ids)}")
        logger.info(f"   –ù–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ –≤ CSV: {len(countries_in_db_not_in_csv)}")
        logger.info(f"   –ü—Ä–æ—Ü–µ–Ω—Ç –æ—Ö–≤–∞—Ç–∞ –ë–î: {(len(found_country_ids)/len(all_db_countries))*100:.1f}%")

        if mapping_used:
            logger.info(f"\nüîÑ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω –º–∞–ø–ø–∏–Ω–≥ –¥–ª—è {len(mapping_used)} —Å—Ç—Ä–∞–Ω:")
            for country_csv in mapping_used[:10]:
                db_names = self.config.country_mapping[country_csv]
                logger.info(f"   '{country_csv}' ‚Üí {db_names}")
            if len(mapping_used) > 10:
                logger.info(f"   ... –∏ –µ—â–µ {len(mapping_used) - 10} —Å—Ç—Ä–∞–Ω")

        # 7. –í—ã–≤–æ–¥–∏–º —Å—Ç—Ä–∞–Ω—ã –∏–∑ CSV, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ –ë–î
        if countries_not_found:
            logger.info(f"\n‚ö†Ô∏è  –°–¢–†–ê–ù–´ –í CSV, –ö–û–¢–û–†–´–• –ù–ï–¢ –í –ë–î ({len(countries_not_found)}):")
            for country in sorted(countries_not_found)[:30]:
                logger.info(f"   - {country}")
            if len(countries_not_found) > 30:
                logger.info(f"   ... –∏ –µ—â–µ {len(countries_not_found) - 30} —Å—Ç—Ä–∞–Ω")

            # –°–æ–∑–¥–∞–µ–º —Ñ–∞–π–ª —Å–æ —Å–ø–∏—Å–∫–æ–º —Å—Ç—Ä–∞–Ω –¥–ª—è —Ä—É—á–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏
            missing_file = Path("logs/missing_countries.txt")
            missing_file.parent.mkdir(exist_ok=True)
            with open(missing_file, "w", encoding="utf-8") as f:
                f.write("# –°—Ç—Ä–∞–Ω—ã –∏–∑ CSV, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ –ë–î\n")
                f.write("# –î–æ–±–∞–≤—å—Ç–µ –∏—Ö –≤ country_mapping –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏\n\n")
                for country in sorted(countries_not_found):
                    f.write(f"# {country}\n")
                    f.write(f"# '{country}': ['{country}'],\n\n")

            logger.info(f"\nüìù –°–ø–∏—Å–æ–∫ –Ω–µ–Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {missing_file}")

        # 8. –í—ã–≤–æ–¥–∏–º —Å—Ç—Ä–∞–Ω—ã –∏–∑ –ë–î, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ CSV (–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ)
        if countries_in_db_not_in_csv:
            logger.info(f"\n‚ÑπÔ∏è  –°–¢–†–ê–ù–´ –í –ë–î, –ö–û–¢–û–†–´–• –ù–ï–¢ –í CSV ({len(countries_in_db_not_in_csv)}):")
            for db_country_name, db_country_id in sorted(countries_in_db_not_in_csv, key=lambda x: x[0])[:50]:
                logger.info(f"   - {db_country_name} (ID: {db_country_id})")
            if len(countries_in_db_not_in_csv) > 50:
                logger.info(f"   ... –∏ –µ—â–µ {len(countries_in_db_not_in_csv) - 50} —Å—Ç—Ä–∞–Ω")

            # –°–æ–∑–¥–∞–µ–º —Ñ–∞–π–ª —Å–æ —Å–ø–∏—Å–∫–æ–º —Å—Ç—Ä–∞–Ω –¥–ª—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
            unused_file = Path("logs/unused_countries.txt")
            unused_file.parent.mkdir(exist_ok=True)
            with open(unused_file, "w", encoding="utf-8") as f:
                f.write("# –°—Ç—Ä–∞–Ω—ã –≤ –ë–î, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –≤ CSV\n")
                f.write("# –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ - –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –¥–µ–π—Å—Ç–≤–∏–π\n\n")
                for db_country_name, db_country_id in sorted(countries_in_db_not_in_csv, key=lambda x: x[0]):
                    f.write(f"# {db_country_name} (ID: {db_country_id})\n")

            logger.info(f"\nüìù –°–ø–∏—Å–æ–∫ –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö —Å—Ç—Ä–∞–Ω —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {unused_file}")

        # 9. –í—ã–≤–æ–¥–∏–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        if countries_not_found:
            logger.info("\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
            logger.info("1. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å –Ω–∞–ø–∏—Å–∞–Ω–∏—è —Å—Ç—Ä–∞–Ω –≤ CSV")
            logger.info("2. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ —Å—Ç—Ä–∞–Ω—ã –≤ –ë–î –∏–º–µ—é—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º (name_eng)")
            logger.info("3. –î–æ–±–∞–≤—å—Ç–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ —Å—Ç—Ä–∞–Ω—ã –≤ –º–∞–ø–ø–∏–Ω–≥ (country_mapping) –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏")
            logger.info("4. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ñ–∞–π–ª missing_countries.txt –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è")

            # –í–ê–ñ–ù–û: –í–æ–∑–≤—Ä–∞—â–∞–µ–º True, —Ç–∞–∫ –∫–∞–∫ —ç—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–∞—è —Å–∏—Ç—É–∞—Ü–∏—è - –Ω–µ –≤—Å–µ —Å—Ç—Ä–∞–Ω—ã –º–æ–≥—É—Ç –±—ã—Ç—å –≤ –ë–î
            # –í —Ä–µ–∞–ª—å–Ω–æ–º ETL –º—ã –º–æ–∂–µ–º –ª–∏–±–æ –ø—Ä–æ–ø—É—Å–∫–∞—Ç—å —ç—Ç–∏ —Å—Ç—Ä–∞–Ω—ã, –ª–∏–±–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–∞–ø–ø–∏–Ω–≥
            logger.info(f"\n‚ö†Ô∏è  –í–Ω–∏–º–∞–Ω–∏–µ: {len(countries_not_found)} —Å—Ç—Ä–∞–Ω –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ –ë–î")
            logger.info("–ò–º–ø–æ—Ä—Ç –±—É–¥–µ—Ç –ø—Ä–æ–¥–æ–ª–∂–µ–Ω –¥–ª—è –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω.")
            return True  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º True, —á—Ç–æ–±—ã ETL –ø—Ä–æ–¥–æ–ª–∂–∞–ª —Ä–∞–±–æ—Ç—É

        logger.info(f"\n‚úÖ –í–°–ï {len(countries_found)} –°–¢–†–ê–ù –ò–ó CSV –ù–ê–ô–î–ï–ù–´ –í –ë–î!")
        logger.info(f"‚úÖ {len(found_country_ids)} –ò–ó {len(all_db_countries)} –°–¢–†–ê–ù –ë–î –ò–°–ü–û–õ–¨–ó–£–Æ–¢–°–Ø –í CSV")
        logger.info(f"\n{'='*60}")
        return True

    async def import_data(self):
        """–ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ CSV –≤ –ë–î"""

        self.stats["start_time"] = datetime.now()

        logger.info(f"\n{'='*60}")
        logger.info(f"–ò–ú–ü–û–†–¢ –î–ê–ù–ù–´–•: {self.config.name}")
        logger.info(f"–§–∞–π–ª: {self.config.csv_file}")
        logger.info(f"{'='*60}")

        csv_path = Path(self.config.csv_file)
        if not csv_path.exists():
            logger.error(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {self.config.csv_file}")
            return

        logger.info("üìñ –ß—Ç–µ–Ω–∏–µ CSV —á–µ—Ä–µ–∑ pandas...")
        df = pd.read_csv(
            csv_path,
            sep=self.config.csv_delimiter,
            encoding=self.config.csv_encoding,
            dtype=str,  # –í—Å–µ –∫–∞–∫ —Å—Ç—Ä–æ–∫–∏ –¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã
            na_filter=False,  # –ù–µ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø—É—Å—Ç—ã–µ –≤ NaN
            keep_default_na=False,
        )

        # –û—á–∏—â–∞–µ–º BOM –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
        df.columns = df.columns.str.replace("\ufeff", "")
        logger.info(f"–û—á–∏—â–µ–Ω–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏: {list(df.columns)}")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        missing_columns = self.config.required_columns - set(df.columns)
        if missing_columns:
            raise ValueError(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {', '.join(missing_columns)}")

        # –°–æ–∑–¥–∞–µ–º/–ø–æ–ª—É—á–∞–µ–º –º–µ—Ç—Ä–∏–∫—É
        metric = await self.db_service.get_or_create_metric(self.config.metric)
        await self.session.commit()
        logger.info(f"‚úÖ –ú–µ—Ç—Ä–∏–∫–∞ —Å–æ–∑–¥–∞–Ω–∞ (ID: {metric.id})")

        batch: List[MetricDataNewModel] = []
        batch_counter = 0
        total_inserted = 0

        for row_idx, (_, row_series) in enumerate(df.iterrows(), 1):
            self.stats["total_rows"] += 1

            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º pandas Series –≤ dict
            row = row_series.to_dict()

            try:
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—Ç—Ä–æ–∫—É –∏ –¥–æ–±–∞–≤–ª—è–µ–º –≤ –±–∞—Ç—á
                await self._process_row_to_batch(row, metric, batch, row_idx)

            except Exception as e:
                self.stats["skipped_rows"] += 1
                error_msg = f"–°—Ç—Ä–æ–∫–∞ {row_idx}: {str(e)}"
                self.stats["errors"].append(error_msg)

                if not self.config.skip_invalid_rows:
                    raise
                else:
                    logger.warning(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–∞ —Å—Ç—Ä–æ–∫–∞ {row_idx}: {str(e)}")

            # –í–°–¢–ê–í–õ–Ø–ï–ú –ë–ê–¢–ß bulk insert'–æ–º
            if len(batch) >= self.config.batch_size:
                try:
                    # Bulk insert –≤—Å–µ–≥–æ –±–∞—Ç—á–∞
                    inserted = await self.db_service.bulk_insert_metric_data(batch)
                    total_inserted += inserted
                    batch_counter += 1

                    logger.info(
                        f"‚úÖ –ë–∞—Ç—á #{batch_counter} —Å–æ—Ö—Ä–∞–Ω–µ–Ω: "
                        f"–ø–æ–ø—ã—Ç–∫–∞ {len(batch)} –∑–∞–ø–∏—Å–µ–π, "
                        f"–≤—Å—Ç–∞–≤–ª–µ–Ω–æ {inserted} –∑–∞–ø–∏—Å–µ–π, "
                        f"–≤—Å–µ–≥–æ –≤—Å—Ç–∞–≤–ª–µ–Ω–æ: {total_inserted}"
                    )

                    # –û—á–∏—â–∞–µ–º –±–∞—Ç—á
                    batch.clear()

                    if batch_counter % 10 == 0:
                        await self.db_service.log_cache_stats()

                except Exception as commit_error:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ bulk insert –±–∞—Ç—á–∞ #{batch_counter}: {commit_error}")
                    await self.session.rollback()
                    raise

        # –í—Å—Ç–∞–≤–∫–∞ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ (–Ω–µ–ø–æ–ª–Ω–æ–≥–æ) –±–∞—Ç—á–∞
        if batch:  # –ï—â–µ –µ—Å—Ç—å –∑–∞–ø–∏—Å–∏ –≤ –±–∞—Ç—á–µ
            try:
                inserted = await self.db_service.bulk_insert_metric_data(batch)
                total_inserted += inserted
                batch_counter += 1
                logger.info(
                    f"‚úÖ –§–∏–Ω–∞–ª—å–Ω—ã–π –±–∞—Ç—á #{batch_counter} —Å–æ—Ö—Ä–∞–Ω–µ–Ω: "
                    f"–ø–æ–ø—ã—Ç–∫–∞ {len(batch)} –∑–∞–ø–∏—Å–µ–π, "
                    f"–≤—Å—Ç–∞–≤–ª–µ–Ω–æ {inserted} –∑–∞–ø–∏—Å–µ–π, "
                    f"–≤—Å–µ–≥–æ –≤—Å—Ç–∞–≤–ª–µ–Ω–æ: {total_inserted}"
                )
                batch.clear()  # –û—á–∏—â–∞–µ–º
            except Exception as commit_error:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ bulk insert: {commit_error}")
                await self.session.rollback()
                raise

        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        self.stats["processed_rows"] = total_inserted
        self.stats["duplicates_skipped"] = self.stats["total_rows"] - total_inserted - self.stats["skipped_rows"]

        # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        self._print_statistics()

    def _print_statistics(self):
        """–í—ã–≤–æ–¥–∏—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏–º–ø–æ—Ä—Ç–∞"""

        self.stats["end_time"] = datetime.now()
        duration = self.stats["end_time"] - self.stats["start_time"]

        db_stats = self.db_service.stats if self.db_service else {"duplicates_skipped": 0, "new_records": 0}

        logger.info(f"\n{'='*60}")
        logger.info("–°–¢–ê–¢–ò–°–¢–ò–ö–ê –ò–ú–ü–û–†–¢–ê")
        logger.info(f"{'='*60}")
        logger.info(f"–í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {duration}")
        logger.info(f"–í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫ –≤ CSV: {self.stats['total_rows']}")
        logger.info(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å—Ç—Ä–æ–∫: {self.stats['processed_rows']}")
        logger.info(f"–ü—Ä–æ–ø—É—â–µ–Ω–æ —Å—Ç—Ä–æ–∫: {self.stats['skipped_rows']}")
        logger.info(f"–î—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø—Ä–æ–ø—É—â–µ–Ω–æ: {db_stats['duplicates_skipped']}")
        logger.info(f"–ù–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π —Å–æ–∑–¥–∞–Ω–æ: {db_stats['new_records']}")

        if self.stats["errors"]:
            logger.info(f"\n–û—à–∏–±–∫–∏ ({len(self.stats['errors'])}):")
            for error in self.stats["errors"][:10]:
                logger.info(f"  - {error}")
            if len(self.stats["errors"]) > 10:
                logger.info(f"  ... –∏ –µ—â–µ {len(self.stats['errors']) - 10} –æ—à–∏–±–æ–∫")

        logger.info(f"\n‚úÖ –ò–º–ø–æ—Ä—Ç –∑–∞–≤–µ—Ä—à–µ–Ω!")

    async def _process_row_to_batch(self, row: Dict[str, str], metric, batch: List[MetricDataNewModel], row_idx: int):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å—Ç—Ä–æ–∫—É –∏ –¥–æ–±–∞–≤–ª—è–µ—Ç –∑–∞–ø–∏—Å—å –≤ –±–∞—Ç—á (–±–µ–∑ –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ–π –≤—Å—Ç–∞–≤–∫–∏)"""

        # row_start = time.time()

        # 1. –ü–æ–ª—É—á–∞–µ–º —Å—Ç—Ä–∞–Ω—É (—Å—É—â–µ—Å—Ç–≤—É—é—â–∞—è –ª–æ–≥–∏–∫–∞)
        country_name = row.get(self.config.metric.country_column, "").strip()
        if not country_name:
            raise ValueError(f"–ü—É—Å—Ç–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ —Å—Ç—Ä–∞–Ω—ã –≤ —Å—Ç—Ä–æ–∫–µ {row_idx}")

        country_id = await self.db_service.get_country_id(
            country_name=country_name,
            country_mapping=self.config.country_mapping,
            column_name=self.config.country_column,
        )

        if not country_id:
            if self.config.validate_country_exists:
                raise ValueError(f"–°—Ç—Ä–∞–Ω–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –ë–î: {country_name}")
            return

        # 2. –ü–æ–ª—É—á–∞–µ–º –≥–æ—Ä–æ–¥ (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
        city_id = None
        if self.config.geography_level == GeographyLevelEnum.CITY and self.config.metric.city_column:
            city_name = row.get(self.config.metric.city_column, "").strip()
            if city_name:
                city_id = await self.db_service.get_city_id(
                    city_name=city_name,
                    country_id=country_id,
                    city_mapping=self.config.city_mapping,
                )
                if not city_id:
                    logger.warning(f"–ì–æ—Ä–æ–¥ –Ω–µ –Ω–∞–π–¥–µ–Ω: {city_name}")

        # 3. –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∞—Ç—Ä–∏–±—É—Ç—ã
        attributes, complex_period_data = await self.db_service.process_attributes(
            row=row,
            attributes_config=self.config.metric.attributes,
        )

        # 4. –ü–æ–ª—É—á–∞–µ–º –∏–ª–∏ —Å–æ–∑–¥–∞–µ–º —Å–µ—Ä–∏—é
        series = await self.db_service.get_or_create_series(
            metric_id=metric.id,
            attributes=attributes,
            series_metadata=self.config.metric.series_metadata,
        )

        # 5. –ü–æ–ª—É—á–∞–µ–º –∏–ª–∏ —Å–æ–∑–¥–∞–µ–º –ø–µ—Ä–∏–æ–¥
        period = await self.db_service.get_or_create_period(
            series_id=cast(int, series.id),
            period_config=self.config.metric.period,
            row=row,
            complex_period_data=complex_period_data,
        )

        # 6. –ü–æ–ª—É—á–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏
        raw_value = row.get(self.config.metric.value_column, "").strip()
        if not raw_value:
            logger.debug(f"–ü—É—Å—Ç–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –≤ —Å—Ç—Ä–æ–∫–µ {row_idx}, –ø—Ä–æ–ø—É—Å–∫")
            return

        # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—é –µ—Å–ª–∏ –µ—Å—Ç—å
        if self.config.metric.value_transform:
            raw_value = self.config.metric.value_transform(raw_value)

        # 7. –°–æ–∑–¥–∞–µ–º –∑–∞–ø–∏—Å—å –¥–∞–Ω–Ω—ã—Ö (–Ω–æ –Ω–µ –≤—Å—Ç–∞–≤–ª—è–µ–º!)
        data_record = await self.db_service.create_metric_data(
            series_id=cast(int, series.id),
            period_id=cast(int, period.id),
            country_id=country_id,
            city_id=city_id,
            value=raw_value,
            data_type=self.config.metric.data_type,
        )

        if data_record:
            batch.append(data_record)

        # logger.debug(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç—Ä–æ–∫–∏ {row_idx}: {time.time() - row_start} —Å–µ–∫")

    async def import_data_parallel(self):
        """–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –∏–º–ø–æ—Ä—Ç —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —á–∞–Ω–∫–æ–≤"""
        self.stats["start_time"] = datetime.now()

        logger.info(f"\n{'='*60}")
        logger.info(f"–ü–ê–†–ê–õ–õ–ï–õ–¨–ù–´–ô –ò–ú–ü–û–†–¢ –î–ê–ù–ù–´–•: {self.config.name}")
        logger.info(f"–§–∞–π–ª: {self.config.csv_file}")
        logger.info(f"{'='*60}")

        csv_path = Path(self.config.csv_file)
        if not csv_path.exists():
            raise FileNotFoundError(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {self.config.csv_file}")

        # –°–æ–∑–¥–∞–µ–º/–ø–æ–ª—É—á–∞–µ–º –º–µ—Ç—Ä–∏–∫—É
        metric = await self.db_service.get_or_create_metric(self.config.metric)
        await self.session.commit()

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞
        file_size = csv_path.stat().st_size
        chunk_size = self._calculate_optimal_chunk_size(file_size)

        logger.info(f"–†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {file_size:,} –±–∞–π—Ç")
        logger.info(f"–†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞: {chunk_size:,} —Å—Ç—Ä–æ–∫")

        # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª —á–∞–Ω–∫–∞–º–∏
        reader = pd.read_csv(
            csv_path,
            sep=self.config.csv_delimiter,
            encoding=self.config.csv_encoding,
            dtype=str,
            na_filter=False,
            chunksize=chunk_size,
            iterator=True,
        )

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —á–∞–Ω–∫–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        tasks = []
        chunk_counter = 0
        total_processed = 0

        for chunk_df in reader:
            chunk_counter += 1

            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —á–∞–Ω–∫ –≤ —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ –≤ –ø—Ä–æ—Ü–µ—Å—Å—ã
            chunk_data = chunk_df.to_dict("records")

            # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á—É –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —á–∞–Ω–∫–∞
            task = asyncio.create_task(
                self._process_chunk_parallel(
                    chunk_data=chunk_data, metric=metric, chunk_number=chunk_counter, start_row=total_processed + 1
                )
            )
            tasks.append(task)

            total_processed += len(chunk_data)

            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –≤—ã–ø–æ–ª–Ω—è–µ–º—ã—Ö –∑–∞–¥–∞—á
            if len(tasks) >= self.config.max_concurrent_chunks or 4:
                processed_count = await self._wait_for_tasks(tasks)
                total_processed += processed_count
                tasks = []

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –∑–∞–¥–∞—á–∏
        if tasks:
            processed_count = await self._wait_for_tasks(tasks)
            total_processed += processed_count

        self.stats["processed_rows"] = total_processed
        self._print_statistics()

    def _calculate_optimal_chunk_size(self, file_size: int) -> int:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞"""
        # –ü—Ä–∏–º–µ—Ä–Ω–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞: 10-100–∫ —Å—Ç—Ä–æ–∫ –Ω–∞ —á–∞–Ω–∫ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–∞–∑–º–µ—Ä–∞ —Ñ–∞–π–ª–∞
        if file_size < 10 * 1024 * 1024:  # < 10MB
            return 10000
        elif file_size < 100 * 1024 * 1024:  # < 100MB
            return 50000
        else:
            return 100000

    async def _wait_for_tasks(self, tasks: List[asyncio.Task]) -> int:
        """–û–∂–∏–¥–∞–µ—Ç –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –∑–∞–¥–∞—á –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Å—Ç—Ä–æ–∫"""
        results = await asyncio.gather(*tasks, return_exceptions=True)

        total_processed = 0
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —á–∞–Ω–∫–∞: {result}")
                if not self.config.skip_invalid_rows:
                    raise result
            else:
                total_processed += result or 0

        return total_processed

    async def _process_chunk_parallel(
        self, chunk_data: List[Dict[str, str]], metric, chunk_number: int, start_row: int
    ) -> int:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —á–∞–Ω–∫ –¥–∞–Ω–Ω—ã—Ö –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø—Ä–æ—Ü–µ—Å—Å–µ"""
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º ProcessPoolExecutor –¥–ª—è CPU-bound –æ–ø–µ—Ä–∞—Ü–∏–π
        with ProcessPoolExecutor(max_workers=1) as executor:
            loop = asyncio.get_event_loop()

            # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø—Ä–æ—Ü–µ—Å—Å–µ
            processed_count = await loop.run_in_executor(
                executor, self._process_chunk_sync, chunk_data, metric.id, chunk_number, start_row
            )

            return processed_count

    def _process_chunk_sync(
        self, chunk_data: List[Dict[str, str]], metric_id: int, chunk_number: int, start_row: int
    ) -> int:
        """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —á–∞–Ω–∫–∞ (–≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø—Ä–æ—Ü–µ—Å—Å–µ)"""
        # –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø—Ä–æ—Ü–µ—Å—Å–µ,
        # –ø–æ—ç—Ç–æ–º—É –Ω—É–∂–Ω–æ —Å–æ–∑–¥–∞—Ç—å –Ω–æ–≤—É—é —Å–µ—Å—Å–∏—é –¥–ª—è —ç—Ç–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞
        import asyncio

        from etl.universal.session_manager import session_manager

        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤–æ–µ —Å–æ–±—ã—Ç–∏–π–Ω–æ–µ loop –¥–ª—è —ç—Ç–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)

        try:
            return loop.run_until_complete(self._process_chunk_async(chunk_data, metric_id, chunk_number, start_row))
        finally:
            loop.close()

    async def _process_chunk_async(
        self, chunk_data: List[Dict[str, str]], metric_id: int, chunk_number: int, start_row: int
    ) -> int:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —á–∞–Ω–∫–∞ —Å –ª–æ–∫–∞–ª—å–Ω–æ–π —Å–µ—Å—Å–∏–µ–π"""
        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é —Å–µ—Å—Å–∏—é –¥–ª—è —ç—Ç–æ–≥–æ —á–∞–Ω–∫–∞
        async with session_manager.get_session() as local_session:
            # –°–æ–∑–¥–∞–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–π —Å–µ—Ä–≤–∏—Å —Å –∫—ç—à–∞–º–∏
            local_db_service = DB_ServiceUniversalETL(local_session, self.config)

            # –ö–æ–ø–∏—Ä—É–µ–º –Ω—É–∂–Ω—ã–µ –∫—ç—à–∏ –∏–∑ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Å–µ—Ä–≤–∏—Å–∞
            await self._sync_caches_to_local(local_db_service)

            batch: List[MetricDataNewModel] = []
            processed_count = 0
            batch_counter = 0

            for i, row in enumerate(chunk_data, 1):
                row_idx = start_row + i - 1

                try:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–π —Å–µ—Ä–≤–∏—Å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
                    await self._process_single_row(
                        row=row, metric_id=metric_id, batch=batch, row_idx=row_idx, db_service=local_db_service
                    )

                except Exception as e:
                    self.stats["skipped_rows"] += 1
                    error_msg = f"–°—Ç—Ä–æ–∫–∞ {row_idx}: {str(e)}"
                    self.stats["errors"].append(error_msg)

                    if not self.config.skip_invalid_rows:
                        raise

                # –í—Å—Ç–∞–≤–ª—è–µ–º –±–∞—Ç—á
                if len(batch) >= self.config.batch_size:
                    try:
                        inserted = await local_db_service.bulk_insert_metric_data(batch)
                        processed_count += inserted
                        batch_counter += 1

                        logger.debug(f"–ß–∞–Ω–∫ {chunk_number}, –±–∞—Ç—á {batch_counter}: " f"–≤—Å—Ç–∞–≤–ª–µ–Ω–æ {inserted} –∑–∞–ø–∏—Å–µ–π")

                        batch.clear()

                    except Exception as e:
                        logger.error(f"–û—à–∏–±–∫–∞ –≤ —á–∞–Ω–∫–µ {chunk_number}, –±–∞—Ç—á–µ {batch_counter}: {e}")
                        raise

            # –í—Å—Ç–∞–≤–ª—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –±–∞—Ç—á
            if batch:
                try:
                    inserted = await local_db_service.bulk_insert_metric_data(batch)
                    processed_count += inserted
                    batch.clear()
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –±–∞—Ç—á–∞ —á–∞–Ω–∫–∞ {chunk_number}: {e}")
                    raise

            await local_session.commit()
            return processed_count

    async def _process_single_row(
        self,
        row: Dict[str, str],
        metric_id: int,
        batch: List[MetricDataNewModel],
        row_idx: int,
        db_service: DB_ServiceUniversalETL,
    ):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø–µ—Ä–µ–¥–∞–Ω–Ω–æ–≥–æ —Å–µ—Ä–≤–∏—Å–∞"""
        # –õ–æ–≥–∏–∫–∞ –∞–Ω–∞–ª–æ–≥–∏—á–Ω–∞—è _process_row_to_batch, –Ω–æ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø–µ—Ä–µ–¥–∞–Ω–Ω–æ–≥–æ —Å–µ—Ä–≤–∏—Å–∞
        country_name = row.get(self.config.metric.country_column, "").strip()
        if not country_name:
            raise ValueError(f"–ü—É—Å—Ç–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ —Å—Ç—Ä–∞–Ω—ã –≤ —Å—Ç—Ä–æ–∫–µ {row_idx}")

        country_id = await db_service.get_country_id(
            country_name=country_name,
            country_mapping=self.config.country_mapping,
            column_name=self.config.country_column,
        )

        if not country_id:
            if self.config.validate_country_exists:
                raise ValueError(f"–°—Ç—Ä–∞–Ω–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –ë–î: {country_name}")
            return

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≥–æ—Ä–æ–¥–∞
        city_id = None
        if self.config.geography_level == GeographyLevelEnum.CITY and self.config.metric.city_column:
            city_name = row.get(self.config.metric.city_column, "").strip()
            if city_name:
                city_id = await db_service.get_city_id(
                    city_name=city_name,
                    country_id=country_id,
                    city_mapping=self.config.city_mapping,
                )

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∞—Ç—Ä–∏–±—É—Ç–æ–≤
        attributes, complex_period_data = await db_service.process_attributes(
            row=row,
            attributes_config=self.config.metric.attributes,
        )

        # –ü–æ–ª—É—á–∞–µ–º –∏–ª–∏ —Å–æ–∑–¥–∞–µ–º —Å–µ—Ä–∏—é
        series = await db_service.get_or_create_series(
            metric_id=metric_id,
            attributes=attributes,
            series_metadata=self.config.metric.series_metadata,
        )

        # –ü–æ–ª—É—á–∞–µ–º –∏–ª–∏ —Å–æ–∑–¥–∞–µ–º –ø–µ—Ä–∏–æ–¥
        period = await db_service.get_or_create_period(
            series_id=cast(int, series.id),
            period_config=self.config.metric.period,
            row=row,
            complex_period_data=complex_period_data,
        )

        # –ü–æ–ª—É—á–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏
        raw_value = row.get(self.config.metric.value_column, "").strip()
        if not raw_value:
            return

        if self.config.metric.value_transform:
            raw_value = self.config.metric.value_transform(raw_value)

        # –°–æ–∑–¥–∞–µ–º –∑–∞–ø–∏—Å—å –¥–∞–Ω–Ω—ã—Ö
        data_record = await db_service.create_metric_data(
            series_id=cast(int, series.id),
            period_id=cast(int, period.id),
            country_id=country_id,
            city_id=city_id,
            value=raw_value,
            data_type=self.config.metric.data_type,
        )

        if data_record:
            batch.append(data_record)

    async def _sync_caches_to_local(self, local_db_service: DB_ServiceUniversalETL):
        """–°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ—Ç –∫—ç—à–∏ –∏–∑ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Å–µ—Ä–≤–∏—Å–∞ –≤ –ª–æ–∫–∞–ª—å–Ω—ã–π"""
        # –ö–æ–ø–∏—Ä—É–µ–º –∫—ç—à–∏ —Å—Ç—Ä–∞–Ω (—Ç–æ–ª—å–∫–æ –∫–ª—é—á–∏, –∑–Ω–∞—á–µ–Ω–∏—è –∑–∞–≥—Ä—É–∑–∏–º –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏)
        # –í —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –ª—É—á—à–µ –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å —Å–µ—Ä–∏–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –∫—ç—à–∏
        pass
